{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>using tutorial <a href=\"https://machinelearningmastery.com/how-to-interpolate-and-perform-vector-arithmetic-with-faces-using-a-generative-adversarial-network/\">here</a>:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#basic keras GAN using celebA\n",
    "\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The discriminator model takes as input one 80×80 color image an outputs a binary prediction as to whether the image is real (<em>class=1</em>) or fake (<em>class=0</em>). It is implemented as a modest convolutional neural network using best practices for GAN design such as using the <a href=\"https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\">LeakyReLU activation function</a> with a slope of 0.2, using a <a href=\"https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/\">2×2 stride to downsample</a>, and the <a href=\"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\">adam version of stochastic gradient descent</a> with a learning rate of 0.0002 and a momentum of 0.5</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and compile standalone discriminator model per above\n",
    "def define_discriminator(in_shape=(80,80,3)):\n",
    "    #all of these 'functions' are from keras, see the first cell above\n",
    "    model = Sequential()\n",
    "    #normal input start\n",
    "    model.add(Conv2D(128, (5,5), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #downsample to 40x40\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #downsample to 20x20\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #downsample to 10x10\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #downsample to 5x5\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #flatten and classify\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #COMPILE MODEL\n",
    "    \n",
    "    #optimizer\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    #compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator model takes as input a point in the latent space and outputs a single 80×80 color image.\n",
    "<br>\n",
    "This is achieved by using a fully connected layer to interpret the point in the latent space and provide sufficient activations that can be reshaped into many copies (in this case 128) of a low-resolution version of the output image (e.g. 5×5). This is then upsampled four times, doubling the size and quadrupling the area of the activations each time using transpose convolutional layers. The model uses best practices such as the LeakyReLU activation, a kernel size that is a factor of the stride size, and a hyperbolic tangent (tanh) activation function in the output layer.\n",
    "<br>\n",
    "The <em>define_generator()</em> function below defines the generator model but intentionally does not compile it as it is not trained directly, then returns the model. The size of the latent space is parameterized as a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and DON'T compile the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    #foundation for 5x5 feature maps  per above\n",
    "    n_nodes = 128 * 5 * 5\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((5, 5, 128)))\n",
    "    #upsample to 10x10\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #upsample to 20x20\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #upsample to 40x40\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #upsample to 80x80\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #output conv2D layer to get RGB?! -> 80x80x3\n",
    "    model.add(Conv2D(3, (5,5), activation='tanh', padding='same'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a GAN model can be defined that combines both the generator model and the discriminator model into one larger model. This larger model will be used to train the model weights in the generator, using the output and error calculated by the discriminator model. The discriminator model is trained separately, and as such, the model weights are marked as not trainable in this larger GAN model to ensure that only the weights of the generator model are updated. This change to the trainability of the discriminator weights only has an effect when training the combined GAN model, not when training the discriminator standalone.\n",
    "\n",
    "This larger GAN model takes as input a point in the latent space, uses the generator model to generate an image, which is fed as input to the discriminator model, then output or classified as real or fake.\n",
    "\n",
    "The <em>define_gan()</em> function below implements this, taking the already-defined generator and discriminator models as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a new combined generator--discriminator model,\n",
    "#the purpose of which is to update the generator kernal (weights?)\n",
    "def define_gan(g_model, d_model):\n",
    "    #make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    #connect the two component models into a new one\n",
    "    model = Sequential()\n",
    "    #add generator\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    #compile umbrella model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before training, now we must import the dataset, then map its multitudinous pixel values to the range [-1,1] from their default input [0,255].\n",
    "<br>\n",
    "the below <em>load_real_samples()</em> function achieves this and returns the entire dataset object likewise prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and normalize the training dataset\n",
    "def load_real_samples():\n",
    "    #load the dataset\n",
    "    data = load('align-80px_celebA_10000set.npz')\n",
    "    X = data['arr_0']\n",
    "    #convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    #map (normalize) data from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each update to the GAN umbrella model is achieved by looking at a batch of real images and comparing them to the generated ones. thus, we need to produce a random batch of images every update.\n",
    "<br>\n",
    "The <em>generate_real_samples()</em> function below implements this, taking the prepared dataset as an argument, selecting and returning a random sample of face images and their corresponding class label for the discriminator, specifically class=1, indicating that they are real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to return random batch of real image samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    #select random instances\n",
    "    #NOTE: constructor randint(low, high, size)\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    #retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    #generate 'real' class labels (literally '1')\n",
    "    #NOTE: constructor ones(shape, dtype)\n",
    "    #which means that the (()) here means that we are specifying an array\n",
    "    #as the shape of the ones-array....slightly odd to see (x,1) but hey\n",
    "    y = ones((n_samples, 1))\n",
    "    #return the random real samples in the form of the images, plus\n",
    "    #a single ones-array indicating their veracity\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we need inputs for the generator model. These are random points from the latent space, specifically <a href=\"https://machinelearningmastery.com/how-to-generate-random-numbers-in-python/\">Gaussian distributed random variables</a>.</p>\n",
    "<p>The <em>generate_latent_points()</em> function below implements this, taking the size of the latent space as an argument and the number of points required and returning them as a batch of input samples for the generator model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make random points in (generator) latent space\n",
    "#as seed inputs for generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    #generate points in the latent space\n",
    "    #NOTE: constructor randn(d0, d1, ... dn) so the shape itself, no other params\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    #reshape into a batch of inputs for the network\n",
    "    #NOTE: constructor numpy.reshape(newshape[int or tuple])\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "#NOTA BENE: strangely enough, this function conjures a horribly\n",
    "#undifferentiated, 1D string of numbers (latent_dim * n_samples) long,\n",
    "#THEN 'reshapes' it into an array with n_samples as the first dimension.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we use use the above-generated random latent points to generate new images, which then get fed into the discriminator in order to go back and train the generator.\n",
    "<br><br>\n",
    "The <em>generate_fake_samples()</em> function below implements this, taking the generator model and size of the latent space as arguments, then generating points in the latent space and using them as input to the generator model. The function returns the generated images and their corresponding class label for the discriminator model, specifically class=0 to indicate they are fake/generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the generator to generate n fake images/samples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    #generate points in latents space via above function\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    #predict outputs\n",
    "    #???? i have no idea what this means but it seems to simply run\n",
    "    #the model without compiling it..? LOOK INTO THIS!!\n",
    "    X = g_model.predict(x_input)\n",
    "    #generate 'fake' class labels (literally '0')\n",
    "    #NOTE: as above, the double parens indicate an array passed as\n",
    "    #the first functional argument, i.e. the shape of the zeros array\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit the GAN models.\n",
    "\n",
    "The model is fit for 100 training epochs, which is arbitrary, as the model begins generating plausible faces after perhaps the first few epochs. A batch size of 128 samples is used, and each training epoch involves 50,000/128 or about 390 batches of real and fake samples and updates to the model.\n",
    "\n",
    "First, the discriminator model is updated for a half batch of real samples, then a half batch of fake samples, together forming one batch of weight updates. The generator is then updated via the combined GAN model. Importantly, the class label is set to 1 or real for the fake samples. This has the effect of updating the generator toward getting better at generating real samples on the next batch.\n",
    "\n",
    "The train() function below implements this, taking the defined models, dataset, and size of the latent dimension as arguments and parameterizing the number of epochs and batch size with default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to TRAIN THE GAN MODEL! per above\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, render_latent, n_epochs=10, n_batch=128):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    \n",
    "    #manually loop epochs\n",
    "    for i in range(n_epochs):\n",
    "        #loop batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            #get half batch of random real samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            #update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            #generate half batch of fake samples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            #update discriminator model weights again\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            #prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            #create inverted labels for the fake samples (i dont get this)\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            #update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            \n",
    "            #summarize loss on this batch\n",
    "            print('>>>>%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "            \n",
    "            #render single latent\n",
    "            render_plot(g_model, render_latent, i, j)\n",
    "            \n",
    "        #evaluate model performance, sometimes\n",
    "        if (i+1) % 1 == 0:\n",
    "            summarize_performance(i, g_model, d_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next section are two handy functions: one to save a plot of n images from the generator, from the tutorial; and one to render a single (constant) latent point, which is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and save a plot of generated images\n",
    "def save_plot(examples, epoch, n=10):\n",
    "    #scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    #plot images\n",
    "    for i in range(n * n):\n",
    "        #define subplot\n",
    "        pyplot.subplot(n, n, 1 + i)\n",
    "        #turn off axes\n",
    "        pyplot.axis('off')\n",
    "        #plot raw pixel data\n",
    "        pyplot.imshow(examples[i])\n",
    "    \n",
    "    #save plot to file\n",
    "    filename = 'generated_plot_e%03d.png' % epoch\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and save a plot of a single latent point, saved with #e## filename\n",
    "def render_plot(g_model, render_latent, epoch, batch):\n",
    "    #as in generate_fake_samples()\n",
    "    render = g_model.predict(render_latent)\n",
    "    #scale from [-1,1] to [0,1]\n",
    "    render = (render + 1) / 2\n",
    "    #plot render\n",
    "    pyplot.axis('off')\n",
    "    pyplot.imshow(render[0])\n",
    "    \n",
    "    #save plot to file\n",
    "    filename = 'render01\\\\render_%03de%03d.png' % (epoch, batch)\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <em>summarize_performance()</em> function generates samples and evaluates the performance of the discriminator on real and fake samples. The classification accuracy is reported and might provide insight into model performance. The <em>save_plot()</em> is called to create and save a plot of the generated images, and then the model is saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate the discriminator, plot generated images, and save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    #prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    #evaluate discriminator on real samples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    #prepare fake samples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    #evaluate discriminator on fake samples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    #summarize and print\n",
    "    print('discriminator accuracy: real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    #save plot\n",
    "    save_plot(x_fake, epoch)\n",
    "    #save generator model tile file\n",
    "    filename = 'generator_model_%03d.h5' % epoch\n",
    "    g_model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, this last cell can be used to define the size of the latent space, define all three models, and train them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of latent space\n",
    "latent_dim = 100\n",
    "#create discriminator\n",
    "d_model = define_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create generator\n",
    "g_model = define_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create gan umbrella model\n",
    "gan_model = define_gan(g_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "dataset = load_real_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i will pause and create a system for <em>rendering</em> the training process as i have done in my other GAN experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_latent = generate_latent_points(latent_dim, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjeff\\Anaconda3\\envs\\neuGAN\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\jjeff\\Anaconda3\\envs\\neuGAN\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>1, 1/78, d1=0.691, d2=0.696, g=0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjeff\\Anaconda3\\envs\\neuGAN\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>1, 2/78, d1=0.518, d2=0.698, g=0.691\n",
      ">>>>1, 3/78, d1=0.172, d2=0.717, g=0.680\n",
      ">>>>1, 4/78, d1=0.011, d2=0.757, g=0.675\n",
      ">>>>1, 5/78, d1=0.003, d2=0.767, g=0.684\n",
      ">>>>1, 6/78, d1=0.009, d2=0.750, g=0.699\n",
      ">>>>1, 7/78, d1=0.028, d2=0.728, g=0.716\n",
      ">>>>1, 8/78, d1=0.037, d2=0.710, g=0.736\n",
      ">>>>1, 9/78, d1=0.026, d2=0.663, g=0.774\n",
      ">>>>1, 10/78, d1=0.008, d2=0.628, g=0.863\n",
      ">>>>1, 11/78, d1=0.006, d2=0.573, g=1.000\n",
      ">>>>1, 12/78, d1=0.017, d2=0.518, g=1.095\n",
      ">>>>1, 13/78, d1=0.000, d2=0.399, g=1.535\n",
      ">>>>1, 14/78, d1=0.000, d2=0.225, g=2.442\n",
      ">>>>1, 15/78, d1=0.000, d2=0.072, g=3.787\n",
      ">>>>1, 16/78, d1=0.250, d2=0.044, g=3.456\n",
      ">>>>1, 17/78, d1=0.000, d2=0.031, g=4.037\n",
      ">>>>1, 18/78, d1=0.000, d2=0.027, g=4.576\n",
      ">>>>1, 19/78, d1=0.000, d2=0.309, g=8.168\n",
      ">>>>1, 20/78, d1=0.000, d2=0.000, g=12.128\n",
      ">>>>1, 21/78, d1=0.000, d2=0.000, g=9.567\n",
      ">>>>1, 22/78, d1=0.000, d2=0.635, g=17.497\n",
      ">>>>1, 23/78, d1=0.061, d2=0.000, g=13.646\n",
      ">>>>1, 24/78, d1=0.316, d2=0.014, g=4.987\n",
      ">>>>1, 25/78, d1=0.000, d2=22.229, g=2.726\n",
      ">>>>1, 26/78, d1=0.000, d2=0.626, g=3.649\n",
      ">>>>1, 27/78, d1=0.038, d2=0.327, g=3.101\n",
      ">>>>1, 28/78, d1=0.019, d2=0.017, g=6.089\n",
      ">>>>1, 29/78, d1=0.177, d2=0.018, g=4.394\n",
      ">>>>1, 30/78, d1=0.347, d2=0.236, g=4.266\n",
      ">>>>1, 31/78, d1=0.028, d2=0.009, g=6.628\n",
      ">>>>1, 32/78, d1=0.088, d2=0.050, g=3.871\n",
      ">>>>1, 33/78, d1=0.042, d2=0.169, g=3.908\n",
      ">>>>1, 34/78, d1=0.530, d2=0.497, g=1.677\n",
      ">>>>1, 35/78, d1=0.003, d2=0.247, g=2.171\n",
      ">>>>1, 36/78, d1=0.030, d2=0.220, g=2.094\n",
      ">>>>1, 37/78, d1=0.077, d2=0.289, g=1.960\n",
      ">>>>1, 38/78, d1=0.634, d2=0.433, g=1.522\n",
      ">>>>1, 39/78, d1=0.002, d2=0.311, g=2.843\n",
      ">>>>1, 40/78, d1=0.064, d2=0.102, g=5.053\n",
      ">>>>1, 41/78, d1=0.116, d2=0.015, g=6.526\n",
      ">>>>1, 42/78, d1=0.011, d2=0.010, g=8.495\n",
      ">>>>1, 43/78, d1=0.010, d2=0.004, g=8.531\n",
      ">>>>1, 44/78, d1=0.008, d2=0.015, g=10.270\n",
      ">>>>1, 45/78, d1=0.006, d2=0.000, g=10.330\n",
      ">>>>1, 46/78, d1=0.003, d2=0.001, g=7.649\n",
      ">>>>1, 47/78, d1=0.020, d2=0.026, g=10.891\n",
      ">>>>1, 48/78, d1=0.006, d2=0.000, g=13.746\n",
      ">>>>1, 49/78, d1=0.001, d2=0.000, g=12.074\n",
      ">>>>1, 50/78, d1=0.007, d2=0.000, g=9.240\n",
      ">>>>1, 51/78, d1=0.002, d2=0.001, g=7.791\n",
      ">>>>1, 52/78, d1=0.001, d2=0.001, g=7.099\n",
      ">>>>1, 53/78, d1=0.003, d2=0.003, g=6.210\n",
      ">>>>1, 54/78, d1=0.005, d2=0.013, g=5.567\n",
      ">>>>1, 55/78, d1=0.001, d2=0.083, g=16.111\n",
      ">>>>1, 56/78, d1=5.219, d2=6.299, g=0.102\n",
      ">>>>1, 57/78, d1=0.000, d2=0.183, g=7.392\n",
      ">>>>1, 58/78, d1=0.004, d2=7.869, g=3.083\n",
      ">>>>1, 59/78, d1=0.003, d2=3.336, g=1.002\n",
      ">>>>1, 60/78, d1=0.129, d2=0.343, g=2.791\n",
      ">>>>1, 61/78, d1=1.318, d2=0.434, g=1.390\n",
      ">>>>1, 62/78, d1=0.064, d2=0.357, g=2.562\n",
      ">>>>1, 63/78, d1=0.092, d2=0.131, g=3.813\n",
      ">>>>1, 64/78, d1=0.144, d2=0.161, g=4.248\n",
      ">>>>1, 65/78, d1=0.171, d2=0.312, g=7.752\n",
      ">>>>1, 66/78, d1=1.273, d2=0.131, g=2.793\n",
      ">>>>1, 67/78, d1=0.337, d2=0.461, g=3.600\n",
      ">>>>1, 68/78, d1=0.324, d2=0.042, g=3.512\n",
      ">>>>1, 69/78, d1=0.588, d2=0.103, g=2.279\n",
      ">>>>1, 70/78, d1=0.195, d2=0.180, g=1.883\n",
      ">>>>1, 71/78, d1=0.139, d2=0.212, g=1.785\n",
      ">>>>1, 72/78, d1=0.066, d2=0.203, g=1.859\n",
      ">>>>1, 73/78, d1=0.067, d2=0.189, g=1.904\n",
      ">>>>1, 74/78, d1=0.239, d2=0.199, g=1.846\n",
      ">>>>1, 75/78, d1=0.207, d2=0.209, g=1.789\n",
      ">>>>1, 76/78, d1=0.031, d2=0.183, g=2.020\n",
      ">>>>1, 77/78, d1=0.001, d2=0.150, g=2.349\n",
      ">>>>1, 78/78, d1=0.139, d2=0.139, g=2.291\n",
      ">>>>2, 1/78, d1=0.003, d2=0.120, g=2.515\n",
      ">>>>2, 2/78, d1=0.076, d2=0.124, g=2.534\n",
      ">>>>2, 3/78, d1=0.267, d2=0.163, g=2.305\n",
      ">>>>2, 4/78, d1=0.071, d2=0.164, g=2.489\n",
      ">>>>2, 5/78, d1=0.017, d2=0.121, g=2.746\n",
      ">>>>2, 6/78, d1=0.004, d2=0.107, g=2.970\n",
      ">>>>2, 7/78, d1=0.167, d2=0.124, g=3.090\n",
      ">>>>2, 8/78, d1=0.003, d2=0.080, g=3.426\n",
      ">>>>2, 9/78, d1=0.009, d2=0.073, g=3.645\n",
      ">>>>2, 10/78, d1=0.007, d2=0.060, g=3.753\n",
      ">>>>2, 11/78, d1=0.006, d2=0.095, g=4.334\n",
      ">>>>2, 12/78, d1=0.001, d2=0.067, g=4.869\n",
      ">>>>2, 13/78, d1=0.037, d2=0.096, g=5.029\n",
      ">>>>2, 14/78, d1=0.077, d2=0.760, g=10.334\n",
      ">>>>2, 15/78, d1=0.001, d2=1.252, g=29.698\n",
      ">>>>2, 16/78, d1=29.405, d2=0.888, g=4.816\n",
      ">>>>2, 17/78, d1=0.002, d2=1.973, g=19.029\n",
      ">>>>2, 18/78, d1=0.736, d2=0.000, g=14.914\n",
      ">>>>2, 19/78, d1=0.218, d2=0.006, g=8.630\n",
      ">>>>2, 20/78, d1=0.086, d2=0.301, g=10.725\n",
      ">>>>2, 21/78, d1=0.094, d2=0.068, g=6.920\n",
      ">>>>2, 22/78, d1=0.113, d2=0.015, g=5.682\n",
      ">>>>2, 23/78, d1=0.208, d2=0.064, g=6.252\n",
      ">>>>2, 24/78, d1=0.068, d2=0.004, g=6.852\n",
      ">>>>2, 25/78, d1=0.057, d2=0.006, g=6.481\n",
      ">>>>2, 26/78, d1=0.010, d2=0.010, g=6.156\n",
      ">>>>2, 27/78, d1=0.071, d2=0.031, g=5.840\n",
      ">>>>2, 28/78, d1=0.059, d2=0.097, g=14.399\n",
      ">>>>2, 29/78, d1=0.017, d2=0.033, g=37.376\n",
      ">>>>2, 30/78, d1=0.044, d2=0.000, g=38.517\n",
      ">>>>2, 31/78, d1=0.015, d2=1.188, g=63.501\n",
      ">>>>2, 32/78, d1=0.166, d2=0.000, g=41.794\n",
      ">>>>2, 33/78, d1=0.151, d2=1.966, g=51.988\n",
      ">>>>2, 34/78, d1=4.701, d2=0.000, g=31.561\n",
      ">>>>2, 35/78, d1=0.720, d2=0.001, g=9.601\n",
      ">>>>2, 36/78, d1=0.386, d2=0.691, g=11.928\n",
      ">>>>2, 37/78, d1=0.518, d2=0.000, g=13.569\n",
      ">>>>2, 38/78, d1=0.347, d2=0.000, g=9.021\n",
      ">>>>2, 39/78, d1=0.097, d2=0.055, g=4.643\n",
      ">>>>2, 40/78, d1=0.455, d2=1.414, g=10.151\n",
      ">>>>2, 41/78, d1=0.564, d2=0.001, g=11.799\n",
      ">>>>2, 42/78, d1=0.677, d2=0.005, g=6.367\n",
      ">>>>2, 43/78, d1=0.088, d2=0.070, g=6.338\n",
      ">>>>2, 44/78, d1=0.251, d2=0.054, g=5.666\n",
      ">>>>2, 45/78, d1=0.110, d2=0.031, g=6.632\n",
      ">>>>2, 46/78, d1=0.293, d2=0.021, g=5.800\n",
      ">>>>2, 47/78, d1=0.226, d2=0.036, g=4.856\n",
      ">>>>2, 48/78, d1=0.098, d2=0.030, g=5.077\n",
      ">>>>2, 49/78, d1=0.011, d2=0.021, g=6.356\n",
      ">>>>2, 50/78, d1=0.050, d2=0.008, g=6.334\n",
      ">>>>2, 51/78, d1=0.035, d2=0.008, g=6.049\n",
      ">>>>2, 52/78, d1=0.268, d2=0.041, g=5.244\n",
      ">>>>2, 53/78, d1=0.099, d2=0.041, g=5.612\n",
      ">>>>2, 54/78, d1=0.182, d2=0.031, g=5.309\n",
      ">>>>2, 55/78, d1=0.024, d2=0.016, g=5.539\n",
      ">>>>2, 56/78, d1=0.020, d2=0.012, g=5.806\n",
      ">>>>2, 57/78, d1=0.183, d2=0.016, g=5.143\n",
      ">>>>2, 58/78, d1=0.129, d2=0.030, g=4.398\n",
      ">>>>2, 59/78, d1=0.019, d2=0.025, g=5.155\n",
      ">>>>2, 60/78, d1=0.016, d2=0.014, g=5.592\n",
      ">>>>2, 61/78, d1=0.023, d2=0.012, g=5.652\n",
      ">>>>2, 62/78, d1=0.016, d2=0.012, g=5.538\n",
      ">>>>2, 63/78, d1=0.007, d2=0.008, g=5.525\n",
      ">>>>2, 64/78, d1=0.026, d2=0.010, g=5.146\n",
      ">>>>2, 65/78, d1=0.004, d2=0.017, g=5.097\n",
      ">>>>2, 66/78, d1=0.010, d2=0.020, g=4.878\n",
      ">>>>2, 67/78, d1=0.025, d2=0.018, g=4.611\n",
      ">>>>2, 68/78, d1=0.061, d2=0.037, g=4.133\n",
      ">>>>2, 69/78, d1=0.051, d2=0.055, g=3.632\n",
      ">>>>2, 70/78, d1=0.016, d2=0.075, g=3.802\n",
      ">>>>2, 71/78, d1=0.003, d2=0.059, g=3.900\n",
      ">>>>2, 72/78, d1=0.131, d2=0.086, g=3.421\n",
      ">>>>2, 73/78, d1=0.025, d2=0.070, g=3.832\n",
      ">>>>2, 74/78, d1=0.109, d2=0.036, g=4.170\n",
      ">>>>2, 75/78, d1=0.024, d2=0.035, g=5.017\n",
      ">>>>2, 76/78, d1=0.006, d2=0.033, g=5.041\n",
      ">>>>2, 77/78, d1=0.012, d2=0.054, g=4.276\n",
      ">>>>2, 78/78, d1=0.045, d2=0.059, g=4.073\n",
      ">>>>3, 1/78, d1=0.074, d2=0.106, g=4.040\n",
      ">>>>3, 2/78, d1=0.026, d2=0.082, g=4.426\n",
      ">>>>3, 3/78, d1=0.072, d2=0.112, g=4.693\n",
      ">>>>3, 4/78, d1=0.054, d2=0.107, g=4.844\n",
      ">>>>3, 5/78, d1=0.240, d2=0.123, g=4.951\n",
      ">>>>3, 6/78, d1=0.298, d2=0.144, g=5.211\n",
      ">>>>3, 7/78, d1=0.018, d2=0.011, g=6.423\n",
      ">>>>3, 8/78, d1=0.024, d2=0.017, g=5.674\n",
      ">>>>3, 9/78, d1=0.027, d2=0.052, g=10.544\n",
      ">>>>3, 10/78, d1=0.206, d2=0.003, g=5.197\n",
      ">>>>3, 11/78, d1=0.002, d2=0.286, g=28.450\n",
      ">>>>3, 12/78, d1=0.776, d2=0.000, g=13.143\n",
      ">>>>3, 13/78, d1=0.088, d2=0.018, g=4.818\n",
      ">>>>3, 14/78, d1=0.026, d2=0.704, g=5.253\n",
      ">>>>3, 15/78, d1=0.018, d2=0.007, g=6.651\n",
      ">>>>3, 16/78, d1=0.012, d2=0.005, g=6.075\n",
      ">>>>3, 17/78, d1=0.298, d2=0.018, g=4.164\n",
      ">>>>3, 18/78, d1=0.026, d2=0.035, g=3.937\n",
      ">>>>3, 19/78, d1=0.014, d2=0.034, g=4.295\n",
      ">>>>3, 20/78, d1=0.020, d2=0.034, g=4.696\n",
      ">>>>3, 21/78, d1=0.003, d2=0.026, g=4.982\n",
      ">>>>3, 22/78, d1=0.052, d2=0.093, g=7.077\n",
      ">>>>3, 23/78, d1=0.231, d2=0.017, g=4.897\n",
      ">>>>3, 24/78, d1=0.018, d2=0.197, g=8.901\n",
      ">>>>3, 25/78, d1=0.281, d2=0.003, g=6.724\n",
      ">>>>3, 26/78, d1=0.139, d2=0.152, g=4.503\n",
      ">>>>3, 27/78, d1=0.127, d2=0.069, g=4.423\n",
      ">>>>3, 28/78, d1=0.249, d2=0.066, g=4.730\n",
      ">>>>3, 29/78, d1=0.043, d2=0.019, g=5.706\n",
      ">>>>3, 30/78, d1=0.066, d2=0.018, g=5.690\n",
      ">>>>3, 31/78, d1=0.073, d2=0.032, g=4.900\n",
      ">>>>3, 32/78, d1=0.202, d2=0.179, g=6.104\n",
      ">>>>3, 33/78, d1=0.059, d2=0.022, g=5.856\n",
      ">>>>3, 34/78, d1=0.041, d2=0.036, g=5.717\n",
      ">>>>3, 35/78, d1=0.035, d2=0.083, g=5.947\n",
      ">>>>3, 36/78, d1=0.213, d2=0.859, g=17.415\n",
      ">>>>3, 37/78, d1=1.384, d2=0.008, g=3.820\n",
      ">>>>3, 38/78, d1=0.072, d2=0.430, g=6.284\n",
      ">>>>3, 39/78, d1=0.394, d2=0.005, g=6.246\n",
      ">>>>3, 40/78, d1=0.113, d2=0.047, g=5.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>3, 41/78, d1=0.053, d2=0.077, g=4.974\n",
      ">>>>3, 42/78, d1=0.096, d2=0.148, g=5.325\n",
      ">>>>3, 43/78, d1=0.099, d2=0.132, g=5.600\n",
      ">>>>3, 44/78, d1=0.372, d2=0.379, g=5.737\n",
      ">>>>3, 45/78, d1=0.435, d2=0.454, g=5.679\n",
      ">>>>3, 46/78, d1=0.239, d2=0.197, g=5.813\n",
      ">>>>3, 47/78, d1=0.362, d2=0.366, g=5.467\n",
      ">>>>3, 48/78, d1=0.381, d2=0.176, g=4.813\n",
      ">>>>3, 49/78, d1=0.087, d2=0.167, g=5.420\n",
      ">>>>3, 50/78, d1=0.146, d2=0.155, g=4.969\n",
      ">>>>3, 51/78, d1=0.169, d2=0.114, g=4.136\n",
      ">>>>3, 52/78, d1=0.138, d2=0.128, g=3.542\n",
      ">>>>3, 53/78, d1=0.068, d2=0.117, g=3.751\n",
      ">>>>3, 54/78, d1=0.059, d2=0.055, g=3.874\n",
      ">>>>3, 55/78, d1=0.019, d2=0.083, g=4.003\n",
      ">>>>3, 56/78, d1=0.002, d2=0.033, g=4.278\n",
      ">>>>3, 57/78, d1=0.018, d2=0.054, g=4.057\n",
      ">>>>3, 58/78, d1=0.007, d2=0.061, g=3.685\n",
      ">>>>3, 59/78, d1=0.055, d2=0.194, g=3.650\n",
      ">>>>3, 60/78, d1=0.087, d2=0.156, g=3.657\n",
      ">>>>3, 61/78, d1=0.001, d2=0.099, g=3.995\n",
      ">>>>3, 62/78, d1=0.015, d2=0.169, g=3.935\n",
      ">>>>3, 63/78, d1=0.058, d2=0.215, g=4.547\n",
      ">>>>3, 64/78, d1=0.173, d2=0.050, g=4.294\n",
      ">>>>3, 65/78, d1=0.174, d2=0.413, g=4.388\n",
      ">>>>3, 66/78, d1=0.148, d2=0.048, g=3.908\n",
      ">>>>3, 67/78, d1=0.084, d2=0.129, g=3.457\n",
      ">>>>3, 68/78, d1=0.012, d2=0.128, g=4.061\n",
      ">>>>3, 69/78, d1=0.032, d2=0.090, g=4.276\n",
      ">>>>3, 70/78, d1=0.112, d2=0.077, g=3.986\n",
      ">>>>3, 71/78, d1=0.111, d2=0.276, g=4.052\n",
      ">>>>3, 72/78, d1=0.105, d2=0.065, g=4.051\n",
      ">>>>3, 73/78, d1=0.027, d2=0.055, g=4.235\n",
      ">>>>3, 74/78, d1=0.084, d2=0.101, g=3.619\n",
      ">>>>3, 75/78, d1=0.087, d2=0.148, g=3.998\n",
      ">>>>3, 76/78, d1=0.085, d2=0.097, g=4.471\n",
      ">>>>3, 77/78, d1=0.221, d2=1.063, g=6.637\n",
      ">>>>3, 78/78, d1=0.627, d2=0.239, g=2.657\n",
      ">>>>4, 1/78, d1=0.006, d2=0.435, g=3.680\n",
      ">>>>4, 2/78, d1=0.022, d2=0.046, g=4.522\n",
      ">>>>4, 3/78, d1=0.105, d2=0.075, g=3.291\n",
      ">>>>4, 4/78, d1=0.035, d2=0.150, g=4.283\n",
      ">>>>4, 5/78, d1=0.039, d2=0.027, g=4.734\n",
      ">>>>4, 6/78, d1=0.099, d2=0.059, g=4.218\n",
      ">>>>4, 7/78, d1=0.030, d2=0.072, g=5.080\n",
      ">>>>4, 8/78, d1=0.094, d2=0.072, g=5.822\n",
      ">>>>4, 9/78, d1=0.143, d2=0.059, g=5.767\n",
      ">>>>4, 10/78, d1=0.032, d2=0.096, g=8.877\n",
      ">>>>4, 11/78, d1=0.231, d2=0.262, g=10.366\n",
      ">>>>4, 12/78, d1=0.076, d2=0.001, g=9.677\n",
      ">>>>4, 13/78, d1=0.082, d2=0.016, g=5.063\n",
      ">>>>4, 14/78, d1=0.021, d2=0.226, g=8.185\n",
      ">>>>4, 15/78, d1=0.342, d2=0.070, g=4.088\n",
      ">>>>4, 16/78, d1=0.026, d2=0.243, g=7.098\n",
      ">>>>4, 17/78, d1=0.111, d2=0.010, g=6.177\n",
      ">>>>4, 18/78, d1=0.198, d2=0.245, g=6.258\n",
      ">>>>4, 19/78, d1=0.160, d2=0.029, g=4.924\n",
      ">>>>4, 20/78, d1=0.125, d2=0.280, g=7.560\n",
      ">>>>4, 21/78, d1=0.411, d2=0.010, g=4.720\n",
      ">>>>4, 22/78, d1=0.064, d2=0.102, g=4.520\n",
      ">>>>4, 23/78, d1=0.080, d2=0.039, g=4.660\n",
      ">>>>4, 24/78, d1=0.165, d2=0.257, g=7.649\n",
      ">>>>4, 25/78, d1=0.119, d2=0.003, g=7.573\n",
      ">>>>4, 26/78, d1=0.215, d2=0.108, g=4.583\n",
      ">>>>4, 27/78, d1=0.026, d2=0.172, g=7.752\n",
      ">>>>4, 28/78, d1=0.243, d2=0.055, g=6.248\n",
      ">>>>4, 29/78, d1=0.225, d2=1.127, g=18.216\n",
      ">>>>4, 30/78, d1=7.189, d2=1.303, g=2.780\n",
      ">>>>4, 31/78, d1=0.122, d2=0.139, g=4.466\n",
      ">>>>4, 32/78, d1=0.189, d2=0.165, g=4.124\n",
      ">>>>4, 33/78, d1=0.254, d2=0.265, g=4.539\n",
      ">>>>4, 34/78, d1=0.340, d2=0.221, g=4.048\n",
      ">>>>4, 35/78, d1=0.258, d2=0.349, g=3.926\n",
      ">>>>4, 36/78, d1=0.345, d2=0.214, g=3.204\n",
      ">>>>4, 37/78, d1=0.169, d2=0.284, g=2.811\n",
      ">>>>4, 38/78, d1=0.233, d2=0.230, g=2.532\n",
      ">>>>4, 39/78, d1=0.159, d2=0.292, g=2.342\n",
      ">>>>4, 40/78, d1=0.157, d2=0.207, g=2.170\n",
      ">>>>4, 41/78, d1=0.064, d2=0.226, g=2.268\n",
      ">>>>4, 42/78, d1=0.118, d2=0.220, g=2.173\n",
      ">>>>4, 43/78, d1=0.137, d2=0.218, g=1.978\n",
      ">>>>4, 44/78, d1=0.067, d2=0.217, g=1.988\n",
      ">>>>4, 45/78, d1=0.057, d2=0.207, g=2.083\n",
      ">>>>4, 46/78, d1=0.094, d2=0.259, g=1.996\n",
      ">>>>4, 47/78, d1=0.028, d2=0.231, g=2.048\n",
      ">>>>4, 48/78, d1=0.081, d2=0.200, g=2.105\n",
      ">>>>4, 49/78, d1=0.060, d2=0.231, g=2.075\n",
      ">>>>4, 50/78, d1=0.075, d2=0.226, g=2.023\n",
      ">>>>4, 51/78, d1=0.056, d2=0.310, g=1.909\n",
      ">>>>4, 52/78, d1=0.068, d2=0.265, g=1.925\n",
      ">>>>4, 53/78, d1=0.082, d2=0.318, g=1.854\n",
      ">>>>4, 54/78, d1=0.058, d2=0.269, g=1.905\n",
      ">>>>4, 55/78, d1=0.112, d2=0.267, g=1.784\n",
      ">>>>4, 56/78, d1=0.049, d2=0.338, g=1.834\n",
      ">>>>4, 57/78, d1=0.080, d2=0.280, g=1.838\n",
      ">>>>4, 58/78, d1=0.065, d2=0.265, g=1.704\n",
      ">>>>4, 59/78, d1=0.023, d2=0.303, g=1.768\n",
      ">>>>4, 60/78, d1=0.103, d2=0.287, g=1.626\n",
      ">>>>4, 61/78, d1=0.006, d2=0.245, g=1.811\n",
      ">>>>4, 62/78, d1=0.015, d2=0.211, g=1.965\n",
      ">>>>4, 63/78, d1=0.007, d2=0.194, g=2.148\n",
      ">>>>4, 64/78, d1=0.034, d2=0.165, g=2.162\n",
      ">>>>4, 65/78, d1=0.075, d2=0.195, g=2.144\n",
      ">>>>4, 66/78, d1=0.091, d2=0.242, g=2.492\n",
      ">>>>4, 67/78, d1=0.130, d2=0.413, g=2.780\n",
      ">>>>4, 68/78, d1=0.049, d2=0.092, g=3.303\n",
      ">>>>4, 69/78, d1=0.033, d2=0.062, g=3.345\n",
      ">>>>4, 70/78, d1=0.020, d2=0.089, g=3.232\n",
      ">>>>4, 71/78, d1=0.207, d2=0.158, g=2.343\n",
      ">>>>4, 72/78, d1=0.008, d2=0.234, g=3.374\n",
      ">>>>4, 73/78, d1=0.064, d2=0.083, g=3.953\n",
      ">>>>4, 74/78, d1=0.161, d2=0.206, g=3.018\n",
      ">>>>4, 75/78, d1=0.005, d2=0.128, g=3.743\n",
      ">>>>4, 76/78, d1=0.156, d2=0.087, g=3.471\n",
      ">>>>4, 77/78, d1=0.017, d2=0.087, g=3.647\n",
      ">>>>4, 78/78, d1=0.243, d2=0.228, g=3.361\n",
      ">>>>5, 1/78, d1=0.037, d2=0.052, g=3.957\n",
      ">>>>5, 2/78, d1=0.003, d2=0.062, g=4.250\n",
      ">>>>5, 3/78, d1=0.004, d2=0.041, g=4.418\n",
      ">>>>5, 4/78, d1=0.020, d2=0.041, g=4.789\n",
      ">>>>5, 5/78, d1=0.060, d2=0.043, g=4.546\n",
      ">>>>5, 6/78, d1=0.019, d2=0.032, g=5.072\n",
      ">>>>5, 7/78, d1=0.010, d2=0.048, g=4.419\n",
      ">>>>5, 8/78, d1=0.048, d2=0.130, g=2.989\n",
      ">>>>5, 9/78, d1=0.069, d2=0.107, g=3.173\n",
      ">>>>5, 10/78, d1=0.005, d2=0.068, g=3.529\n",
      ">>>>5, 11/78, d1=0.007, d2=0.056, g=3.680\n",
      ">>>>5, 12/78, d1=0.012, d2=0.042, g=3.990\n",
      ">>>>5, 13/78, d1=0.083, d2=0.114, g=5.175\n",
      ">>>>5, 14/78, d1=0.046, d2=0.021, g=4.279\n",
      ">>>>5, 15/78, d1=0.024, d2=0.070, g=5.047\n",
      ">>>>5, 16/78, d1=0.052, d2=0.061, g=5.415\n",
      ">>>>5, 17/78, d1=0.141, d2=1.583, g=18.247\n",
      ">>>>5, 18/78, d1=7.767, d2=0.404, g=3.151\n",
      ">>>>5, 19/78, d1=0.083, d2=11.072, g=6.324\n",
      ">>>>5, 20/78, d1=0.812, d2=0.109, g=2.746\n",
      ">>>>5, 21/78, d1=0.749, d2=1.371, g=9.705\n",
      ">>>>5, 22/78, d1=2.871, d2=0.017, g=2.919\n",
      ">>>>5, 23/78, d1=0.179, d2=0.121, g=2.184\n",
      ">>>>5, 24/78, d1=0.169, d2=0.170, g=2.260\n",
      ">>>>5, 25/78, d1=0.070, d2=0.133, g=2.434\n",
      ">>>>5, 26/78, d1=0.052, d2=0.124, g=2.699\n",
      ">>>>5, 27/78, d1=0.053, d2=0.097, g=3.113\n",
      ">>>>5, 28/78, d1=0.094, d2=0.079, g=3.197\n",
      ">>>>5, 29/78, d1=0.098, d2=0.133, g=3.543\n",
      ">>>>5, 30/78, d1=0.081, d2=0.091, g=3.798\n",
      ">>>>5, 31/78, d1=0.202, d2=0.124, g=3.810\n",
      ">>>>5, 32/78, d1=0.164, d2=0.165, g=4.776\n",
      ">>>>5, 33/78, d1=0.360, d2=0.082, g=4.404\n",
      ">>>>5, 34/78, d1=0.352, d2=0.178, g=4.508\n",
      ">>>>5, 35/78, d1=0.406, d2=0.051, g=4.226\n",
      ">>>>5, 36/78, d1=0.226, d2=0.071, g=3.274\n",
      ">>>>5, 37/78, d1=0.249, d2=0.209, g=4.358\n",
      ">>>>5, 38/78, d1=0.296, d2=0.026, g=4.261\n",
      ">>>>5, 39/78, d1=0.185, d2=0.057, g=3.633\n",
      ">>>>5, 40/78, d1=0.117, d2=0.087, g=3.387\n",
      ">>>>5, 41/78, d1=0.283, d2=0.136, g=3.063\n",
      ">>>>5, 42/78, d1=0.214, d2=0.129, g=2.741\n",
      ">>>>5, 43/78, d1=0.172, d2=0.136, g=2.823\n",
      ">>>>5, 44/78, d1=0.176, d2=0.183, g=2.628\n",
      ">>>>5, 45/78, d1=0.103, d2=0.120, g=2.941\n",
      ">>>>5, 46/78, d1=0.306, d2=0.162, g=2.532\n",
      ">>>>5, 47/78, d1=0.250, d2=0.204, g=2.346\n",
      ">>>>5, 48/78, d1=0.136, d2=0.216, g=2.601\n",
      ">>>>5, 49/78, d1=0.183, d2=0.194, g=2.526\n",
      ">>>>5, 50/78, d1=0.208, d2=0.339, g=2.696\n",
      ">>>>5, 51/78, d1=0.142, d2=0.504, g=3.446\n",
      ">>>>5, 52/78, d1=0.354, d2=0.475, g=3.726\n",
      ">>>>5, 53/78, d1=0.608, d2=0.356, g=3.635\n",
      ">>>>5, 54/78, d1=0.616, d2=0.552, g=3.569\n",
      ">>>>5, 55/78, d1=0.459, d2=0.426, g=3.292\n",
      ">>>>5, 56/78, d1=0.596, d2=0.556, g=3.127\n",
      ">>>>5, 57/78, d1=0.976, d2=0.201, g=2.655\n",
      ">>>>5, 58/78, d1=0.542, d2=0.286, g=2.655\n",
      ">>>>5, 59/78, d1=0.495, d2=0.248, g=2.512\n",
      ">>>>5, 60/78, d1=0.435, d2=0.331, g=2.531\n",
      ">>>>5, 61/78, d1=0.400, d2=0.493, g=2.509\n",
      ">>>>5, 62/78, d1=0.445, d2=0.377, g=2.708\n",
      ">>>>5, 63/78, d1=0.482, d2=0.179, g=2.591\n",
      ">>>>5, 64/78, d1=0.315, d2=0.169, g=2.487\n",
      ">>>>5, 65/78, d1=0.259, d2=0.245, g=2.679\n",
      ">>>>5, 66/78, d1=0.230, d2=0.251, g=2.905\n",
      ">>>>5, 67/78, d1=0.363, d2=0.467, g=2.779\n",
      ">>>>5, 68/78, d1=0.563, d2=0.393, g=2.757\n",
      ">>>>5, 69/78, d1=0.508, d2=0.219, g=2.761\n",
      ">>>>5, 70/78, d1=0.564, d2=0.164, g=2.149\n",
      ">>>>5, 71/78, d1=0.383, d2=0.363, g=2.275\n",
      ">>>>5, 72/78, d1=0.367, d2=0.222, g=2.453\n",
      ">>>>5, 73/78, d1=0.405, d2=0.239, g=2.272\n",
      ">>>>5, 74/78, d1=0.333, d2=0.316, g=2.358\n",
      ">>>>5, 75/78, d1=0.327, d2=0.247, g=2.320\n",
      ">>>>5, 76/78, d1=0.246, d2=0.207, g=2.550\n",
      ">>>>5, 77/78, d1=0.266, d2=0.262, g=2.409\n",
      ">>>>5, 78/78, d1=0.255, d2=0.210, g=2.426\n",
      ">>>>6, 1/78, d1=0.200, d2=0.232, g=2.774\n",
      ">>>>6, 2/78, d1=0.277, d2=0.218, g=2.704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>6, 3/78, d1=0.286, d2=0.161, g=2.997\n",
      ">>>>6, 4/78, d1=0.280, d2=0.205, g=2.985\n",
      ">>>>6, 5/78, d1=0.169, d2=0.158, g=3.214\n",
      ">>>>6, 6/78, d1=0.285, d2=0.130, g=2.923\n",
      ">>>>6, 7/78, d1=0.127, d2=0.241, g=3.348\n",
      ">>>>6, 8/78, d1=0.165, d2=0.080, g=3.569\n",
      ">>>>6, 9/78, d1=0.235, d2=0.372, g=4.202\n",
      ">>>>6, 10/78, d1=0.442, d2=0.091, g=3.120\n",
      ">>>>6, 11/78, d1=0.264, d2=0.591, g=4.035\n",
      ">>>>6, 12/78, d1=0.634, d2=0.252, g=2.857\n",
      ">>>>6, 13/78, d1=0.392, d2=0.288, g=2.598\n",
      ">>>>6, 14/78, d1=0.282, d2=0.190, g=2.822\n",
      ">>>>6, 15/78, d1=0.244, d2=0.268, g=3.187\n",
      ">>>>6, 16/78, d1=0.285, d2=0.216, g=3.046\n",
      ">>>>6, 17/78, d1=0.196, d2=0.176, g=3.084\n",
      ">>>>6, 18/78, d1=0.279, d2=0.324, g=3.395\n",
      ">>>>6, 19/78, d1=0.317, d2=0.141, g=3.049\n",
      ">>>>6, 20/78, d1=0.272, d2=0.221, g=2.612\n",
      ">>>>6, 21/78, d1=0.223, d2=0.227, g=2.872\n",
      ">>>>6, 22/78, d1=0.263, d2=0.220, g=3.061\n",
      ">>>>6, 23/78, d1=0.220, d2=0.142, g=2.953\n",
      ">>>>6, 24/78, d1=0.209, d2=0.161, g=2.658\n",
      ">>>>6, 25/78, d1=0.120, d2=0.182, g=2.973\n",
      ">>>>6, 26/78, d1=0.197, d2=0.153, g=2.819\n",
      ">>>>6, 27/78, d1=0.170, d2=0.164, g=2.781\n",
      ">>>>6, 28/78, d1=0.130, d2=0.152, g=2.843\n",
      ">>>>6, 29/78, d1=0.180, d2=0.205, g=2.691\n",
      ">>>>6, 30/78, d1=0.109, d2=0.235, g=3.043\n",
      ">>>>6, 31/78, d1=0.152, d2=0.111, g=2.948\n",
      ">>>>6, 32/78, d1=0.210, d2=0.370, g=2.975\n",
      ">>>>6, 33/78, d1=0.172, d2=0.210, g=3.236\n",
      ">>>>6, 34/78, d1=0.218, d2=0.197, g=3.068\n",
      ">>>>6, 35/78, d1=0.142, d2=0.209, g=3.100\n",
      ">>>>6, 36/78, d1=0.119, d2=0.106, g=3.153\n",
      ">>>>6, 37/78, d1=0.127, d2=0.163, g=3.339\n",
      ">>>>6, 38/78, d1=0.105, d2=0.108, g=3.654\n",
      ">>>>6, 39/78, d1=0.137, d2=0.114, g=3.307\n",
      ">>>>6, 40/78, d1=0.060, d2=0.095, g=3.699\n",
      ">>>>6, 41/78, d1=0.164, d2=0.100, g=3.466\n",
      ">>>>6, 42/78, d1=0.094, d2=0.111, g=3.325\n",
      ">>>>6, 43/78, d1=0.128, d2=0.147, g=3.483\n",
      ">>>>6, 44/78, d1=0.126, d2=0.145, g=3.503\n",
      ">>>>6, 45/78, d1=0.211, d2=0.210, g=3.395\n",
      ">>>>6, 46/78, d1=0.174, d2=0.137, g=3.261\n",
      ">>>>6, 47/78, d1=0.113, d2=0.248, g=4.049\n",
      ">>>>6, 48/78, d1=0.321, d2=0.162, g=3.235\n",
      ">>>>6, 49/78, d1=0.161, d2=0.242, g=4.216\n",
      ">>>>6, 50/78, d1=0.386, d2=0.256, g=4.216\n",
      ">>>>6, 51/78, d1=0.279, d2=0.196, g=3.852\n",
      ">>>>6, 52/78, d1=0.227, d2=0.210, g=3.972\n",
      ">>>>6, 53/78, d1=0.196, d2=0.236, g=4.427\n",
      ">>>>6, 54/78, d1=0.212, d2=0.195, g=4.289\n",
      ">>>>6, 55/78, d1=0.424, d2=0.626, g=4.634\n",
      ">>>>6, 56/78, d1=0.460, d2=0.146, g=3.924\n",
      ">>>>6, 57/78, d1=0.130, d2=0.115, g=4.110\n",
      ">>>>6, 58/78, d1=0.193, d2=0.135, g=3.911\n",
      ">>>>6, 59/78, d1=0.236, d2=0.183, g=3.901\n",
      ">>>>6, 60/78, d1=0.134, d2=0.130, g=3.986\n",
      ">>>>6, 61/78, d1=0.157, d2=0.121, g=3.961\n",
      ">>>>6, 62/78, d1=0.105, d2=0.118, g=4.170\n",
      ">>>>6, 63/78, d1=0.153, d2=0.263, g=4.447\n",
      ">>>>6, 64/78, d1=0.243, d2=0.096, g=3.479\n",
      ">>>>6, 65/78, d1=0.161, d2=0.728, g=6.805\n",
      ">>>>6, 66/78, d1=1.949, d2=0.610, g=2.267\n",
      ">>>>6, 67/78, d1=0.070, d2=0.132, g=3.161\n",
      ">>>>6, 68/78, d1=0.201, d2=0.136, g=3.004\n",
      ">>>>6, 69/78, d1=0.088, d2=0.195, g=3.384\n",
      ">>>>6, 70/78, d1=0.234, d2=0.455, g=4.429\n",
      ">>>>6, 71/78, d1=0.618, d2=0.116, g=2.736\n",
      ">>>>6, 72/78, d1=0.224, d2=0.302, g=2.946\n",
      ">>>>6, 73/78, d1=0.273, d2=0.205, g=2.992\n",
      ">>>>6, 74/78, d1=0.137, d2=0.130, g=3.299\n",
      ">>>>6, 75/78, d1=0.231, d2=0.149, g=2.995\n",
      ">>>>6, 76/78, d1=0.124, d2=0.192, g=3.294\n",
      ">>>>6, 77/78, d1=0.135, d2=0.085, g=3.296\n",
      ">>>>6, 78/78, d1=0.195, d2=0.169, g=3.023\n",
      ">>>>7, 1/78, d1=0.102, d2=0.128, g=3.199\n",
      ">>>>7, 2/78, d1=0.123, d2=0.119, g=3.180\n",
      ">>>>7, 3/78, d1=0.143, d2=0.184, g=3.269\n",
      ">>>>7, 4/78, d1=0.105, d2=0.132, g=3.708\n",
      ">>>>7, 5/78, d1=0.223, d2=0.376, g=4.900\n",
      ">>>>7, 6/78, d1=0.700, d2=0.058, g=2.786\n",
      ">>>>7, 7/78, d1=0.079, d2=0.230, g=2.877\n",
      ">>>>7, 8/78, d1=0.152, d2=0.169, g=3.286\n",
      ">>>>7, 9/78, d1=0.188, d2=0.235, g=3.489\n",
      ">>>>7, 10/78, d1=0.323, d2=0.157, g=3.120\n",
      ">>>>7, 11/78, d1=0.223, d2=0.338, g=3.604\n",
      ">>>>7, 12/78, d1=0.196, d2=0.123, g=3.663\n",
      ">>>>7, 13/78, d1=0.332, d2=0.302, g=3.122\n",
      ">>>>7, 14/78, d1=0.173, d2=0.166, g=3.462\n",
      ">>>>7, 15/78, d1=0.278, d2=0.216, g=2.852\n",
      ">>>>7, 16/78, d1=0.255, d2=0.445, g=4.244\n",
      ">>>>7, 17/78, d1=0.717, d2=0.315, g=3.317\n",
      ">>>>7, 18/78, d1=0.346, d2=0.612, g=4.046\n",
      ">>>>7, 19/78, d1=0.269, d2=0.102, g=3.488\n",
      ">>>>7, 20/78, d1=0.151, d2=0.295, g=3.564\n",
      ">>>>7, 21/78, d1=0.105, d2=0.097, g=3.565\n",
      ">>>>7, 22/78, d1=0.163, d2=0.137, g=3.141\n",
      ">>>>7, 23/78, d1=0.102, d2=0.247, g=3.548\n",
      ">>>>7, 24/78, d1=0.144, d2=0.074, g=3.769\n",
      ">>>>7, 25/78, d1=0.093, d2=0.180, g=3.996\n",
      ">>>>7, 26/78, d1=0.314, d2=0.266, g=3.586\n",
      ">>>>7, 27/78, d1=0.210, d2=0.118, g=3.428\n",
      ">>>>7, 28/78, d1=0.124, d2=0.132, g=3.413\n",
      ">>>>7, 29/78, d1=0.196, d2=0.294, g=3.989\n",
      ">>>>7, 30/78, d1=0.346, d2=0.129, g=3.491\n",
      ">>>>7, 31/78, d1=0.153, d2=0.158, g=3.364\n",
      ">>>>7, 32/78, d1=0.055, d2=0.114, g=4.128\n",
      ">>>>7, 33/78, d1=0.219, d2=0.134, g=3.860\n",
      ">>>>7, 34/78, d1=0.096, d2=0.117, g=4.280\n",
      ">>>>7, 35/78, d1=0.192, d2=0.211, g=4.900\n",
      ">>>>7, 36/78, d1=0.350, d2=0.134, g=4.114\n",
      ">>>>7, 37/78, d1=0.303, d2=0.218, g=4.234\n",
      ">>>>7, 38/78, d1=0.156, d2=0.179, g=4.494\n",
      ">>>>7, 39/78, d1=0.209, d2=0.180, g=3.663\n",
      ">>>>7, 40/78, d1=0.366, d2=0.488, g=4.184\n",
      ">>>>7, 41/78, d1=0.367, d2=0.248, g=4.198\n",
      ">>>>7, 42/78, d1=0.223, d2=0.202, g=4.582\n",
      ">>>>7, 43/78, d1=0.167, d2=0.154, g=4.477\n",
      ">>>>7, 44/78, d1=0.313, d2=0.317, g=5.005\n",
      ">>>>7, 45/78, d1=0.486, d2=0.195, g=4.148\n",
      ">>>>7, 46/78, d1=0.231, d2=0.132, g=3.672\n",
      ">>>>7, 47/78, d1=0.248, d2=0.386, g=3.724\n",
      ">>>>7, 48/78, d1=0.316, d2=0.185, g=3.342\n",
      ">>>>7, 49/78, d1=0.111, d2=0.167, g=3.903\n",
      ">>>>7, 50/78, d1=0.122, d2=0.100, g=4.101\n",
      ">>>>7, 51/78, d1=0.160, d2=0.098, g=3.856\n",
      ">>>>7, 52/78, d1=0.066, d2=0.055, g=4.257\n",
      ">>>>7, 53/78, d1=0.082, d2=0.098, g=4.209\n",
      ">>>>7, 54/78, d1=0.031, d2=0.086, g=4.829\n",
      ">>>>7, 55/78, d1=0.104, d2=0.074, g=4.331\n",
      ">>>>7, 56/78, d1=0.074, d2=0.196, g=5.548\n",
      ">>>>7, 57/78, d1=0.165, d2=0.064, g=5.199\n",
      ">>>>7, 58/78, d1=0.052, d2=0.085, g=6.353\n",
      ">>>>7, 59/78, d1=0.193, d2=0.032, g=5.080\n",
      ">>>>7, 60/78, d1=0.127, d2=0.283, g=7.125\n",
      ">>>>7, 61/78, d1=0.133, d2=0.021, g=5.838\n",
      ">>>>7, 62/78, d1=0.044, d2=0.128, g=4.610\n",
      ">>>>7, 63/78, d1=0.287, d2=0.113, g=4.803\n",
      ">>>>7, 64/78, d1=0.007, d2=0.034, g=5.592\n",
      ">>>>7, 65/78, d1=0.022, d2=0.056, g=5.562\n",
      ">>>>7, 66/78, d1=0.020, d2=0.032, g=5.316\n",
      ">>>>7, 67/78, d1=0.229, d2=0.177, g=6.415\n",
      ">>>>7, 68/78, d1=0.025, d2=0.010, g=7.053\n",
      ">>>>7, 69/78, d1=0.077, d2=0.195, g=6.114\n",
      ">>>>7, 70/78, d1=0.057, d2=0.039, g=5.753\n",
      ">>>>7, 71/78, d1=0.116, d2=0.044, g=5.786\n",
      ">>>>7, 72/78, d1=0.095, d2=0.023, g=5.805\n",
      ">>>>7, 73/78, d1=0.018, d2=0.029, g=5.903\n",
      ">>>>7, 74/78, d1=0.025, d2=0.144, g=7.272\n",
      ">>>>7, 75/78, d1=0.074, d2=0.266, g=9.103\n",
      ">>>>7, 76/78, d1=0.305, d2=0.541, g=11.736\n",
      ">>>>7, 77/78, d1=0.873, d2=0.000, g=8.983\n",
      ">>>>7, 78/78, d1=0.052, d2=0.002, g=6.013\n",
      ">>>>8, 1/78, d1=0.057, d2=0.034, g=3.787\n",
      ">>>>8, 2/78, d1=0.043, d2=0.115, g=3.507\n",
      ">>>>8, 3/78, d1=0.021, d2=0.211, g=4.489\n",
      ">>>>8, 4/78, d1=0.041, d2=0.149, g=5.839\n",
      ">>>>8, 5/78, d1=0.111, d2=0.166, g=7.371\n",
      ">>>>8, 6/78, d1=0.307, d2=0.047, g=6.048\n",
      ">>>>8, 7/78, d1=0.173, d2=0.059, g=5.165\n",
      ">>>>8, 8/78, d1=0.281, d2=0.141, g=5.925\n",
      ">>>>8, 9/78, d1=0.023, d2=0.020, g=6.714\n",
      ">>>>8, 10/78, d1=0.046, d2=0.018, g=6.281\n",
      ">>>>8, 11/78, d1=0.158, d2=0.105, g=5.100\n",
      ">>>>8, 12/78, d1=0.094, d2=0.021, g=4.976\n",
      ">>>>8, 13/78, d1=0.281, d2=0.176, g=4.674\n",
      ">>>>8, 14/78, d1=0.010, d2=0.077, g=5.493\n",
      ">>>>8, 15/78, d1=0.014, d2=1.218, g=12.765\n",
      ">>>>8, 16/78, d1=1.739, d2=0.132, g=4.028\n",
      ">>>>8, 17/78, d1=0.108, d2=1.292, g=5.902\n",
      ">>>>8, 18/78, d1=1.136, d2=0.057, g=4.303\n",
      ">>>>8, 19/78, d1=0.240, d2=0.120, g=3.626\n",
      ">>>>8, 20/78, d1=0.160, d2=0.248, g=3.475\n",
      ">>>>8, 21/78, d1=0.291, d2=0.370, g=3.689\n",
      ">>>>8, 22/78, d1=0.266, d2=0.213, g=3.989\n",
      ">>>>8, 23/78, d1=0.323, d2=0.170, g=3.645\n",
      ">>>>8, 24/78, d1=0.407, d2=0.204, g=3.342\n",
      ">>>>8, 25/78, d1=0.388, d2=0.192, g=3.572\n",
      ">>>>8, 26/78, d1=0.267, d2=0.088, g=3.631\n",
      ">>>>8, 27/78, d1=0.213, d2=0.095, g=3.626\n",
      ">>>>8, 28/78, d1=0.203, d2=0.124, g=3.411\n",
      ">>>>8, 29/78, d1=0.135, d2=0.097, g=3.659\n",
      ">>>>8, 30/78, d1=0.142, d2=0.096, g=3.670\n",
      ">>>>8, 31/78, d1=0.156, d2=0.083, g=3.210\n",
      ">>>>8, 32/78, d1=0.088, d2=0.144, g=3.589\n",
      ">>>>8, 33/78, d1=0.273, d2=0.133, g=3.586\n",
      ">>>>8, 34/78, d1=0.260, d2=0.294, g=3.830\n",
      ">>>>8, 35/78, d1=0.356, d2=0.110, g=3.320\n",
      ">>>>8, 36/78, d1=0.182, d2=0.196, g=3.875\n",
      ">>>>8, 37/78, d1=0.209, d2=0.121, g=3.905\n",
      ">>>>8, 38/78, d1=0.294, d2=0.145, g=3.296\n",
      ">>>>8, 39/78, d1=0.186, d2=0.161, g=3.798\n",
      ">>>>8, 40/78, d1=0.223, d2=0.136, g=3.461\n",
      ">>>>8, 41/78, d1=0.109, d2=0.115, g=3.847\n",
      ">>>>8, 42/78, d1=0.273, d2=0.270, g=3.905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>8, 43/78, d1=0.239, d2=0.189, g=3.856\n",
      ">>>>8, 44/78, d1=0.183, d2=0.199, g=3.917\n",
      ">>>>8, 45/78, d1=0.085, d2=0.188, g=3.922\n",
      ">>>>8, 46/78, d1=0.391, d2=0.242, g=3.334\n",
      ">>>>8, 47/78, d1=0.373, d2=0.219, g=3.461\n",
      ">>>>8, 48/78, d1=0.232, d2=0.155, g=3.770\n",
      ">>>>8, 49/78, d1=0.148, d2=0.212, g=3.747\n",
      ">>>>8, 50/78, d1=0.310, d2=0.156, g=3.199\n",
      ">>>>8, 51/78, d1=0.217, d2=0.216, g=3.678\n",
      ">>>>8, 52/78, d1=0.253, d2=0.146, g=4.172\n",
      ">>>>8, 53/78, d1=0.222, d2=0.107, g=3.540\n",
      ">>>>8, 54/78, d1=0.139, d2=0.157, g=3.309\n",
      ">>>>8, 55/78, d1=0.189, d2=0.122, g=3.152\n",
      ">>>>8, 56/78, d1=0.100, d2=0.209, g=3.863\n",
      ">>>>8, 57/78, d1=0.110, d2=0.077, g=3.986\n",
      ">>>>8, 58/78, d1=0.189, d2=0.157, g=3.528\n",
      ">>>>8, 59/78, d1=0.169, d2=0.142, g=3.549\n",
      ">>>>8, 60/78, d1=0.294, d2=0.143, g=3.240\n",
      ">>>>8, 61/78, d1=0.151, d2=0.175, g=3.694\n",
      ">>>>8, 62/78, d1=0.236, d2=0.109, g=3.846\n",
      ">>>>8, 63/78, d1=0.255, d2=0.111, g=3.643\n",
      ">>>>8, 64/78, d1=0.106, d2=0.158, g=4.426\n",
      ">>>>8, 65/78, d1=0.281, d2=0.093, g=3.389\n",
      ">>>>8, 66/78, d1=0.126, d2=0.105, g=3.349\n",
      ">>>>8, 67/78, d1=0.132, d2=0.078, g=3.647\n",
      ">>>>8, 68/78, d1=0.104, d2=0.090, g=3.811\n",
      ">>>>8, 69/78, d1=0.245, d2=0.166, g=3.808\n",
      ">>>>8, 70/78, d1=0.133, d2=0.091, g=3.985\n",
      ">>>>8, 71/78, d1=0.119, d2=0.101, g=3.979\n",
      ">>>>8, 72/78, d1=0.090, d2=0.076, g=4.023\n",
      ">>>>8, 73/78, d1=0.087, d2=0.166, g=4.376\n",
      ">>>>8, 74/78, d1=0.223, d2=0.196, g=3.951\n",
      ">>>>8, 75/78, d1=0.193, d2=0.157, g=4.225\n",
      ">>>>8, 76/78, d1=0.164, d2=0.114, g=4.670\n",
      ">>>>8, 77/78, d1=0.508, d2=0.741, g=4.706\n",
      ">>>>8, 78/78, d1=0.272, d2=0.053, g=4.327\n",
      ">>>>9, 1/78, d1=0.273, d2=0.208, g=3.458\n",
      ">>>>9, 2/78, d1=0.082, d2=0.106, g=4.148\n",
      ">>>>9, 3/78, d1=0.127, d2=0.061, g=4.167\n",
      ">>>>9, 4/78, d1=0.116, d2=0.062, g=4.235\n",
      ">>>>9, 5/78, d1=0.231, d2=0.095, g=4.308\n",
      ">>>>9, 6/78, d1=0.075, d2=0.057, g=4.949\n",
      ">>>>9, 7/78, d1=0.015, d2=0.031, g=5.061\n",
      ">>>>9, 8/78, d1=0.021, d2=0.038, g=4.942\n",
      ">>>>9, 9/78, d1=0.032, d2=0.055, g=4.981\n",
      ">>>>9, 10/78, d1=0.016, d2=0.026, g=4.847\n",
      ">>>>9, 11/78, d1=0.067, d2=0.070, g=4.500\n",
      ">>>>9, 12/78, d1=0.036, d2=0.091, g=4.173\n",
      ">>>>9, 13/78, d1=0.001, d2=0.091, g=4.271\n",
      ">>>>9, 14/78, d1=0.086, d2=0.231, g=6.545\n",
      ">>>>9, 15/78, d1=0.236, d2=0.023, g=5.779\n",
      ">>>>9, 16/78, d1=0.307, d2=0.324, g=5.451\n",
      ">>>>9, 17/78, d1=0.256, d2=0.027, g=4.303\n",
      ">>>>9, 18/78, d1=0.070, d2=0.322, g=5.696\n",
      ">>>>9, 19/78, d1=0.807, d2=0.325, g=3.955\n",
      ">>>>9, 20/78, d1=0.340, d2=0.359, g=4.520\n",
      ">>>>9, 21/78, d1=0.290, d2=0.092, g=3.734\n",
      ">>>>9, 22/78, d1=0.271, d2=0.562, g=5.098\n",
      ">>>>9, 23/78, d1=0.495, d2=0.107, g=4.472\n",
      ">>>>9, 24/78, d1=0.313, d2=0.210, g=3.009\n",
      ">>>>9, 25/78, d1=0.079, d2=0.216, g=3.690\n",
      ">>>>9, 26/78, d1=0.220, d2=0.173, g=3.781\n",
      ">>>>9, 27/78, d1=0.169, d2=0.115, g=3.614\n",
      ">>>>9, 28/78, d1=0.204, d2=0.121, g=3.625\n",
      ">>>>9, 29/78, d1=0.095, d2=0.104, g=3.896\n",
      ">>>>9, 30/78, d1=0.225, d2=0.112, g=3.849\n",
      ">>>>9, 31/78, d1=0.126, d2=0.078, g=4.263\n",
      ">>>>9, 32/78, d1=0.190, d2=0.194, g=4.115\n",
      ">>>>9, 33/78, d1=0.055, d2=0.122, g=4.904\n",
      ">>>>9, 34/78, d1=0.217, d2=0.112, g=4.746\n",
      ">>>>9, 35/78, d1=0.125, d2=0.109, g=4.995\n",
      ">>>>9, 36/78, d1=0.270, d2=0.139, g=4.794\n",
      ">>>>9, 37/78, d1=0.159, d2=0.034, g=4.696\n",
      ">>>>9, 38/78, d1=0.087, d2=0.057, g=4.318\n",
      ">>>>9, 39/78, d1=0.083, d2=0.274, g=8.465\n",
      ">>>>9, 40/78, d1=0.749, d2=0.020, g=4.417\n",
      ">>>>9, 41/78, d1=0.144, d2=0.741, g=7.035\n",
      ">>>>9, 42/78, d1=0.666, d2=0.049, g=3.936\n",
      ">>>>9, 43/78, d1=0.322, d2=0.729, g=7.024\n",
      ">>>>9, 44/78, d1=0.682, d2=0.015, g=5.031\n",
      ">>>>9, 45/78, d1=0.344, d2=0.156, g=2.908\n",
      ">>>>9, 46/78, d1=0.109, d2=0.307, g=3.635\n",
      ">>>>9, 47/78, d1=0.125, d2=0.082, g=4.010\n",
      ">>>>9, 48/78, d1=0.260, d2=0.160, g=3.461\n",
      ">>>>9, 49/78, d1=0.296, d2=0.205, g=3.437\n",
      ">>>>9, 50/78, d1=0.144, d2=0.171, g=3.530\n",
      ">>>>9, 51/78, d1=0.312, d2=0.240, g=3.419\n",
      ">>>>9, 52/78, d1=0.195, d2=0.191, g=3.493\n",
      ">>>>9, 53/78, d1=0.210, d2=0.197, g=3.550\n",
      ">>>>9, 54/78, d1=0.268, d2=0.182, g=3.501\n",
      ">>>>9, 55/78, d1=0.509, d2=0.342, g=3.228\n",
      ">>>>9, 56/78, d1=0.241, d2=0.127, g=3.146\n",
      ">>>>9, 57/78, d1=0.219, d2=0.175, g=3.114\n",
      ">>>>9, 58/78, d1=0.137, d2=0.126, g=3.404\n",
      ">>>>9, 59/78, d1=0.136, d2=0.097, g=3.620\n",
      ">>>>9, 60/78, d1=0.175, d2=0.145, g=3.728\n",
      ">>>>9, 61/78, d1=0.232, d2=0.138, g=3.764\n",
      ">>>>9, 62/78, d1=0.308, d2=0.292, g=4.000\n",
      ">>>>9, 63/78, d1=0.192, d2=0.060, g=4.049\n",
      ">>>>9, 64/78, d1=0.168, d2=0.088, g=3.663\n",
      ">>>>9, 65/78, d1=0.134, d2=0.202, g=4.807\n",
      ">>>>9, 66/78, d1=0.144, d2=0.024, g=4.870\n",
      ">>>>9, 67/78, d1=0.239, d2=0.103, g=3.471\n",
      ">>>>9, 68/78, d1=0.192, d2=0.297, g=4.497\n",
      ">>>>9, 69/78, d1=0.233, d2=0.069, g=3.933\n",
      ">>>>9, 70/78, d1=0.273, d2=0.534, g=5.239\n",
      ">>>>9, 71/78, d1=0.554, d2=0.081, g=3.592\n",
      ">>>>9, 72/78, d1=0.155, d2=0.205, g=3.235\n",
      ">>>>9, 73/78, d1=0.180, d2=0.179, g=3.551\n",
      ">>>>9, 74/78, d1=0.349, d2=0.190, g=3.390\n",
      ">>>>9, 75/78, d1=0.115, d2=0.177, g=3.791\n",
      ">>>>9, 76/78, d1=0.259, d2=0.144, g=3.692\n",
      ">>>>9, 77/78, d1=0.119, d2=0.091, g=3.784\n",
      ">>>>9, 78/78, d1=0.136, d2=0.240, g=4.320\n",
      ">>>>10, 1/78, d1=0.157, d2=0.096, g=4.094\n",
      ">>>>10, 2/78, d1=0.325, d2=0.340, g=3.609\n",
      ">>>>10, 3/78, d1=0.160, d2=0.102, g=4.032\n",
      ">>>>10, 4/78, d1=0.278, d2=0.147, g=3.392\n",
      ">>>>10, 5/78, d1=0.098, d2=0.109, g=3.707\n",
      ">>>>10, 6/78, d1=0.196, d2=0.135, g=4.229\n",
      ">>>>10, 7/78, d1=0.153, d2=0.064, g=3.753\n",
      ">>>>10, 8/78, d1=0.060, d2=0.161, g=4.775\n",
      ">>>>10, 9/78, d1=0.138, d2=0.036, g=4.265\n",
      ">>>>10, 10/78, d1=0.162, d2=0.250, g=4.599\n",
      ">>>>10, 11/78, d1=0.202, d2=0.091, g=3.895\n",
      ">>>>10, 12/78, d1=0.160, d2=0.210, g=4.284\n",
      ">>>>10, 13/78, d1=0.241, d2=0.098, g=3.825\n",
      ">>>>10, 14/78, d1=0.136, d2=0.234, g=4.440\n",
      ">>>>10, 15/78, d1=0.166, d2=0.200, g=4.907\n",
      ">>>>10, 16/78, d1=0.177, d2=0.088, g=4.587\n",
      ">>>>10, 17/78, d1=0.205, d2=0.135, g=3.688\n",
      ">>>>10, 18/78, d1=0.082, d2=0.143, g=4.056\n",
      ">>>>10, 19/78, d1=0.185, d2=0.179, g=4.223\n",
      ">>>>10, 20/78, d1=0.176, d2=0.261, g=5.211\n",
      ">>>>10, 21/78, d1=0.238, d2=0.062, g=4.045\n",
      ">>>>10, 22/78, d1=0.095, d2=0.343, g=5.827\n",
      ">>>>10, 23/78, d1=0.326, d2=0.077, g=3.480\n",
      ">>>>10, 24/78, d1=0.128, d2=0.522, g=6.372\n",
      ">>>>10, 25/78, d1=0.941, d2=0.101, g=2.307\n",
      ">>>>10, 26/78, d1=0.019, d2=0.911, g=5.109\n",
      ">>>>10, 27/78, d1=0.322, d2=0.021, g=4.534\n",
      ">>>>10, 28/78, d1=0.557, d2=0.403, g=2.567\n",
      ">>>>10, 29/78, d1=0.122, d2=0.467, g=3.696\n",
      ">>>>10, 30/78, d1=0.206, d2=0.144, g=4.209\n",
      ">>>>10, 31/78, d1=0.581, d2=0.403, g=4.046\n",
      ">>>>10, 32/78, d1=0.144, d2=0.079, g=4.601\n",
      ">>>>10, 33/78, d1=0.366, d2=0.266, g=4.583\n",
      ">>>>10, 34/78, d1=0.213, d2=0.049, g=4.541\n",
      ">>>>10, 35/78, d1=0.241, d2=0.259, g=6.297\n",
      ">>>>10, 36/78, d1=0.538, d2=0.149, g=4.296\n",
      ">>>>10, 37/78, d1=0.090, d2=0.133, g=4.902\n",
      ">>>>10, 38/78, d1=0.383, d2=0.112, g=3.733\n",
      ">>>>10, 39/78, d1=0.225, d2=0.380, g=5.085\n",
      ">>>>10, 40/78, d1=0.539, d2=0.205, g=3.225\n",
      ">>>>10, 41/78, d1=0.137, d2=0.577, g=4.483\n",
      ">>>>10, 42/78, d1=0.508, d2=0.155, g=3.644\n",
      ">>>>10, 43/78, d1=0.269, d2=0.329, g=4.393\n",
      ">>>>10, 44/78, d1=0.301, d2=0.077, g=4.300\n",
      ">>>>10, 45/78, d1=0.295, d2=0.321, g=5.540\n",
      ">>>>10, 46/78, d1=0.226, d2=0.099, g=5.057\n",
      ">>>>10, 47/78, d1=0.208, d2=1.099, g=6.775\n",
      ">>>>10, 48/78, d1=1.200, d2=0.126, g=2.188\n",
      ">>>>10, 49/78, d1=0.393, d2=0.492, g=1.718\n",
      ">>>>10, 50/78, d1=0.280, d2=0.345, g=2.230\n",
      ">>>>10, 51/78, d1=0.278, d2=0.260, g=2.771\n",
      ">>>>10, 52/78, d1=0.200, d2=0.179, g=3.295\n",
      ">>>>10, 53/78, d1=0.156, d2=0.115, g=3.533\n",
      ">>>>10, 54/78, d1=0.245, d2=0.149, g=3.128\n",
      ">>>>10, 55/78, d1=0.136, d2=0.212, g=3.139\n",
      ">>>>10, 56/78, d1=0.254, d2=0.166, g=2.828\n",
      ">>>>10, 57/78, d1=0.100, d2=0.200, g=3.258\n",
      ">>>>10, 58/78, d1=0.239, d2=0.357, g=4.415\n",
      ">>>>10, 59/78, d1=0.545, d2=0.201, g=3.449\n",
      ">>>>10, 60/78, d1=0.324, d2=0.207, g=3.510\n",
      ">>>>10, 61/78, d1=0.246, d2=0.139, g=3.566\n",
      ">>>>10, 62/78, d1=0.149, d2=0.184, g=3.847\n",
      ">>>>10, 63/78, d1=0.298, d2=0.127, g=3.321\n",
      ">>>>10, 64/78, d1=0.106, d2=0.094, g=3.628\n",
      ">>>>10, 65/78, d1=0.193, d2=0.127, g=3.614\n",
      ">>>>10, 66/78, d1=0.075, d2=0.080, g=4.094\n",
      ">>>>10, 67/78, d1=0.134, d2=0.098, g=3.770\n",
      ">>>>10, 68/78, d1=0.191, d2=0.171, g=3.861\n",
      ">>>>10, 69/78, d1=0.191, d2=0.088, g=3.922\n",
      ">>>>10, 70/78, d1=0.098, d2=0.145, g=4.367\n",
      ">>>>10, 71/78, d1=0.154, d2=0.092, g=4.101\n",
      ">>>>10, 72/78, d1=0.199, d2=0.168, g=4.267\n",
      ">>>>10, 73/78, d1=0.197, d2=0.058, g=3.944\n",
      ">>>>10, 74/78, d1=0.088, d2=0.113, g=4.439\n",
      ">>>>10, 75/78, d1=0.176, d2=0.081, g=4.272\n",
      ">>>>10, 76/78, d1=0.109, d2=0.099, g=4.411\n",
      ">>>>10, 77/78, d1=0.190, d2=0.096, g=4.298\n",
      ">>>>10, 78/78, d1=0.257, d2=0.196, g=4.654\n",
      "discriminator accuracy: real: 97%, fake: 99%\n"
     ]
    }
   ],
   "source": [
    "#TRAIN!!!!\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim, render_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
